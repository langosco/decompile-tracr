{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import sys\n",
    "from collections import defaultdict \n",
    "import jax\n",
    "import flax\n",
    "import chex\n",
    "from jaxtyping import ArrayLike\n",
    "from typing import Union, TypeVar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from tracr.compiler.validating import validate\n",
    "from tracr.rasp.rasp import Map, SequenceMap, LinearSequenceMap, Select, Aggregate, Comparison, SelectorWidth, indices, tokens \n",
    "from tracr.rasp import rasp\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler.assemble import AssembledTransformerModel\n",
    "from tracr.compiler.craft_model_to_transformer import NoTokensError\n",
    "from tracr.compiler.basis_inference import InvalidValueSetError\n",
    "from tracr.compiler import rasp_to_graph\n",
    "\n",
    "\n",
    "from decompile_tracr.dataset import lib\n",
    "from decompile_tracr.dataset import data_utils\n",
    "from decompile_tracr.dataset import config\n",
    "from decompile_tracr.tokenizing import tokenizer\n",
    "from decompile_tracr.tokenizing import vocab\n",
    "from decompile_tracr.sampling import sampling\n",
    "from decompile_tracr.sampling import rasp_utils\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_DATA_RATIO = 0.1\n",
    "MAX_RASP_LENGTH = config.MAX_RASP_LENGTH\n",
    "MAX_WEIGHTS_LENGTH = config.MAX_WEIGHTS_LENGTH\n",
    "FULL_DATA_DIR = config.full_dataset_dir\n",
    "ALL_LAYERS_MULTIPLIER = 15\n",
    "split_layers = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-04 18:57:45 - [INFO]: Loading data from /home/lauro/projects/metamodels/decompile-tracr/data/full.\n",
      "2024-04-04 18:57:45 - [WARNING]: load_batches: Loaded 413 < 5000 datapoints.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_process_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloaddir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFULL_DATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rasp_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_RASP_LENGTH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mMAX_RASP_LENGTH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mALL_LAYERS_MULTIPLIER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_weights_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_WEIGHTS_LENGTH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mMAX_WEIGHTS_LENGTH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mALL_LAYERS_MULTIPLIER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/metamodels/decompile-tracr/decompile_tracr/dataset/data_utils.py:44\u001b[0m, in \u001b[0;36mload_and_process_data\u001b[0;34m(rng, loaddir, max_data, shuffle, name, d_model, max_rasp_len, max_weights_len, split_layers)\u001b[0m\n\u001b[1;32m     42\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloaddir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m data \u001b[38;5;241m=\u001b[39m load_batches(loaddir\u001b[38;5;241m=\u001b[39mloaddir, max_data\u001b[38;5;241m=\u001b[39mmax_data)\n\u001b[0;32m---> 44\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mflatten_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m     47\u001b[0m     rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(rng)\n",
      "File \u001b[0;32m~/projects/metamodels/decompile-tracr/decompile_tracr/dataset/data_utils.py:73\u001b[0m, in \u001b[0;36mflatten_data\u001b[0;34m(data, split_layers)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m program_id, program \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[1;32m     72\u001b[0m     n_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(program[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(program[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m n_layers\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split_layers:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m w, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(program[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m], program[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = data_utils.load_and_process_data(\n",
    "    rng=rng,\n",
    "    loaddir=FULL_DATA_DIR,\n",
    "    max_data=5000,\n",
    "    shuffle=True,\n",
    "    name=\"train\",\n",
    "    d_model=128,\n",
    "    max_rasp_len=MAX_RASP_LENGTH if split_layers else MAX_RASP_LENGTH * ALL_LAYERS_MULTIPLIER,\n",
    "    max_weights_len=MAX_WEIGHTS_LENGTH if split_layers else MAX_WEIGHTS_LENGTH * ALL_LAYERS_MULTIPLIER,\n",
    "    split_layers=split_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['tokens', 'weights', 'program_id', 'n_sops', 'n_layers'])\n",
      "data shapes: {'tokens': (96, 480), 'weights': (96, 960, 128), 'program_id': (96,), 'n_sops': (96,), 'n_layers': (96,)}\n"
     ]
    }
   ],
   "source": [
    "print(\"keys:\", data.keys())\n",
    "print(\"data shapes:\", {k: v.shape for k, v in data.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96/96 unique tokens (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates among tokens\n",
    "tokens = data[\"tokens\"]\n",
    "unique_tokens = defaultdict(list)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    t = tuple(token.tolist())\n",
    "    unique_tokens[t].append(i)\n",
    "\n",
    "print(f\"Found {len(unique_tokens)}/{len(tokens)} unique tokens \"\n",
    "      f\"({100 * len(unique_tokens) / len(tokens):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero tokens: 6.3%\n"
     ]
    }
   ],
   "source": [
    "# number of non-padding tokens\n",
    "print(f\"Number of nonzero tokens: {(tokens > 0).sum() / tokens.size * 100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical sops: 84.3%\n",
      "Numerical sops: 15.7%\n",
      "Total sops: 306\n"
     ]
    }
   ],
   "source": [
    "# distribution of token types\n",
    "\n",
    "# encodings\n",
    "cat, num = tokenizer.encode([\"categorical\", \"numerical\"])\n",
    "n_categorical = (tokens == cat).sum()\n",
    "n_numerical = (tokens == num).sum()\n",
    "total = n_categorical + n_numerical\n",
    "\n",
    "print(f\"Categorical sops: {100*n_categorical/total:0.1f}%\")\n",
    "print(f\"Numerical sops: {100*n_numerical/total:0.1f}%\")\n",
    "print(f\"Total sops: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation counts:\n",
      "Map: 14.7%\n",
      "SequenceMap: 30.4%\n",
      "LinearSequenceMap: 1.0%\n",
      "SelectAggregate: 18.3%\n",
      "SelectorWidth: 35.6%\n",
      "Total SOps: 306\n"
     ]
    }
   ],
   "source": [
    "ops = tokenizer.encode(vocab.ops)\n",
    "op_counts = {vocab.vocab[op]: (tokens == op).sum() for op in ops}\n",
    "total = sum(op_counts.values())\n",
    "\n",
    "print(\"Operation counts:\")\n",
    "for op, count in op_counts.items():\n",
    "    print(f\"{op}: {100*count/total:.1f}%\")\n",
    "\n",
    "print(f\"Total SOps: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programs = [tokenizer.detokenize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_inputs_and_outputs(\n",
    "    programs: list[rasp.SOp],\n",
    "    n_samples: int = 50,\n",
    "):\n",
    "    \"\"\"Generate test inputs and pass forward through programs to get outputs.\"\"\"\n",
    "    test_inputs = [rasp_utils.sample_test_input(rng, max_seq_len=5, \n",
    "                                    min_seq_len=5, vocab=set(range(10))) \n",
    "                for _ in range(n_samples)]\n",
    "    outputs = [[p(x) for x in test_inputs] for p in programs]\n",
    "    outputs = np.array(outputs, dtype=float)\n",
    "    outputs = np.nan_to_num(outputs, nan=0.0)\n",
    "    return test_inputs, outputs\n",
    "\n",
    "\n",
    "def test_low_var(outputs: list):\n",
    "    \"\"\"Test that sampled programs have a reasonable amount of variance wrt input\"\"\"\n",
    "    stds = np.std(outputs, axis=1).sum(axis=-1)  # std across test inputs; sum across output sequence\n",
    "    are_low_var = stds < 0.01\n",
    "    frac_low_var = sum(are_low_var) / len(stds)\n",
    "    print(f\"{frac_low_var*100}% of programs have low variance in output.\")\n",
    "\n",
    "\n",
    "inputs, outputs = get_test_inputs_and_outputs()\n",
    "test_low_var(outputs)\n",
    "\n",
    "\n",
    "\n",
    "outputs_buffer = outputs.copy()\n",
    "\n",
    "data = []\n",
    "for i, p in enumerate(programs):\n",
    "    data.append({\n",
    "        \"program\": p,\n",
    "        \"outputs\": outputs[i],\n",
    "        \"std\": np.std(outputs[i], axis=0).sum(),\n",
    "    })\n",
    "\n",
    "\n",
    "# sort by std\n",
    "by_std = sorted(data, key=lambda x: np.std(x['outputs']))\n",
    "len(by_std)\n",
    "by_std = iter(by_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = next(by_std)\n",
    "print(\"std:\", np.std(p['outputs']))\n",
    "print()\n",
    "print('input: ', inputs[0])\n",
    "rasp_utils.print_program(p['program'], test_input=inputs[0], full=True)\n",
    "print()\n",
    "print('sample outputs:', p['outputs'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96/96 unique model params (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates among tokens\n",
    "weights = data[\"weights\"]\n",
    "unique = defaultdict(list)\n",
    "duplicate_weights = []\n",
    "\n",
    "for i, w in enumerate(weights):\n",
    "    w = tuple(w.flatten().tolist())\n",
    "    if w in unique:\n",
    "        duplicate_weights.append(i)\n",
    "    \n",
    "    unique[w].append(i)\n",
    "\n",
    "print(f\"Found {len(unique)}/{len(weights)} unique model params \"\n",
    "      f\"({100 * len(unique) / len(weights):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent padding: 93.7%\n",
      "percent zero: 6.1%\n",
      "left over: 0.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"percent padding: {100 * (weights == 0.05).sum() / weights.size:0.1f}%\")\n",
    "print(f\"percent zero: {100 * (weights == 0).sum() / weights.size:0.1f}%\")\n",
    "print(f\"left over: {100 * np.logical_and(weights != 0, weights != 0.05).sum() / weights.size:0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent padding: 93.1%\n",
      "percent zero: 6.7%\n",
      "left over: 0.2%\n"
     ]
    }
   ],
   "source": [
    "def get_percentages(idx):\n",
    "    w = data[\"weights\"][idx]\n",
    "\n",
    "    print(f\"percent padding: {100 * (w == 0.05).sum() / w.size:0.1f}%\")\n",
    "    print(f\"percent zero: {100 * (w == 0).sum() / w.size:0.1f}%\")\n",
    "    print(f\"left over: {100 * np.logical_and(w != 0, w != 0.05).sum() / w.size:0.1f}%\")\n",
    "\n",
    "\n",
    "def plot_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    w = w.flatten()\n",
    "\n",
    "    plt.plot(w, \".\")\n",
    "    plt.yscale(\"symlog\", linthresh=0.1)\n",
    "\n",
    "    print(\" \".join(tokenizer.decode(t)))\n",
    "\n",
    "\n",
    "def imshow_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    _, d_model = w.shape\n",
    "    w = w.flatten()\n",
    "\n",
    "    is_padding = w == 0.05\n",
    "    first_padding_idx = is_padding.tolist().index(True)\n",
    "    idx = first_padding_idx + (d_model - first_padding_idx % d_model)\n",
    "    reshaped_w = w[:idx].reshape(-1, d_model)\n",
    "    reshaped_w[reshaped_w == 0] = np.nan\n",
    "    plt.imshow(reshaped_w, aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "get_percentages(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "#plot_datapoint(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dupe_idx in duplicate_weights:\n",
    "    w = weights[dupe_idx]\n",
    "    t = tokens[dupe_idx]\n",
    "\n",
    "    duplicates = unique[tuple(w.flatten().tolist())]\n",
    "\n",
    "    if not all([np.all(tokens[i] == tokens[dupe_idx]) for i in duplicates]):\n",
    "        print(f\"dupe idx: {dupe_idx}\")\n",
    "        print(\"Found duplicates with different tokens:\")\n",
    "        for i in duplicates:\n",
    "            print(\" \".join(tokenizer.decode(tokens[i])))\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for close duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:04<00:00, 20.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for w in tqdm(weights):\n",
    "    close = [np.allclose(w, u) for u in unique.values()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
