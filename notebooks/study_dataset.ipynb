{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import sys\n",
    "from collections import defaultdict \n",
    "import jax\n",
    "import flax\n",
    "import chex\n",
    "from jaxtyping import ArrayLike\n",
    "from typing import Union, TypeVar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from tracr.compiler.validating import validate\n",
    "from tracr.rasp.rasp import Map, SequenceMap, LinearSequenceMap, Select, Aggregate, Comparison, SelectorWidth, indices, tokens \n",
    "from tracr.rasp import rasp\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler.assemble import AssembledTransformerModel\n",
    "from tracr.compiler.craft_model_to_transformer import NoTokensError\n",
    "from tracr.compiler.basis_inference import InvalidValueSetError\n",
    "from tracr.compiler import rasp_to_graph\n",
    "\n",
    "\n",
    "from decompile_tracr.dataset import lib\n",
    "from decompile_tracr.dataset import data_utils\n",
    "from decompile_tracr.dataset import config\n",
    "from decompile_tracr.tokenizing import tokenizer\n",
    "from decompile_tracr.tokenizing import vocab\n",
    "from decompile_tracr.sampling import sampling\n",
    "from decompile_tracr.sampling import rasp_utils\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-26 14:34:36 - [INFO]: Loading data from /home/lauro/projects/meta-models/decompile-tracr/data/full.h5.\n",
      "2024-04-26 14:34:36 - [INFO]: load_dataset_for_model_input: All data loading and processing took 0.06s.\n"
     ]
    }
   ],
   "source": [
    "with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "    data = data_utils.load_dataset_for_model_input(\n",
    "        loadfile=config.data_dir / \"full.h5\",\n",
    "        ndata=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: ['layer_idx', 'n_layers', 'n_sops', 'tokens', 'weights']\n",
      "data shapes: {'layer_idx': (50, 25), 'n_layers': (50,), 'n_sops': (50,), 'tokens': (50, 256), 'weights': (50, 65536)}\n"
     ]
    }
   ],
   "source": [
    "print(\"keys:\", list(data.keys()))\n",
    "print(\"data shapes:\", {k: v.shape for k, v in data.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50/50 unique tokenized programs (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates among tokens\n",
    "tokens = data[\"tokens\"]\n",
    "unique_tokens = defaultdict(list)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    t = tuple(token.tolist())\n",
    "    unique_tokens[t].append(i)\n",
    "\n",
    "print(f\"Found {len(unique_tokens)}/{len(tokens)} unique tokenized programs \"\n",
    "      f\"({100 * len(unique_tokens) / len(tokens):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_sops_sorted(toks):\n",
    "    decoded = tokenizer.decode(toks)\n",
    "    sops = [x for x in decoded if x.startswith(\"sop_\") \n",
    "            and not (\"tokens\" in x or \"indices\" in x)]\n",
    "    first_occurences = []\n",
    "    for sop in sops:\n",
    "        if sop not in first_occurences:\n",
    "            first_occurences.append(sop)\n",
    "    assert sorted(first_occurences) == first_occurences, first_occurences\n",
    "\n",
    "\n",
    "for i, toks in enumerate(tokens):\n",
    "    assert_sops_sorted(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS',\n",
       " 'sop_00',\n",
       " 'categorical',\n",
       " 'SelectAggregate',\n",
       " 'sop_10_tokens',\n",
       " 'sop_10_tokens',\n",
       " 'EQ',\n",
       " 'sop_10_tokens',\n",
       " 'EOO',\n",
       " 'EOL',\n",
       " 'sop_01',\n",
       " 'numerical',\n",
       " 'Map',\n",
       " 'lambda x: x + 0.5',\n",
       " 'sop_10_tokens',\n",
       " 'EOO',\n",
       " 'EOL',\n",
       " 'sop_02',\n",
       " 'categorical',\n",
       " 'SelectAggregate',\n",
       " 'sop_10_tokens',\n",
       " 'sop_00',\n",
       " 'EQ',\n",
       " 'sop_00',\n",
       " 'EOO',\n",
       " 'EOL',\n",
       " 'sop_03',\n",
       " 'categorical',\n",
       " 'SequenceMap',\n",
       " 'lambda x, y: x * y',\n",
       " 'sop_02',\n",
       " 'sop_00',\n",
       " 'EOO',\n",
       " 'sop_04',\n",
       " 'categorical',\n",
       " 'SequenceMap',\n",
       " 'lambda x, y: x + y % 10',\n",
       " 'sop_02',\n",
       " 'sop_10_tokens',\n",
       " 'EOO',\n",
       " 'EOL',\n",
       " 'EOL',\n",
       " 'sop_05',\n",
       " 'numerical',\n",
       " 'Map',\n",
       " 'lambda x: x > 3',\n",
       " 'sop_04',\n",
       " 'EOO',\n",
       " 'EOL',\n",
       " 'sop_06',\n",
       " 'numerical',\n",
       " 'SelectAggregate',\n",
       " 'sop_11_indices',\n",
       " 'sop_03',\n",
       " 'NEQ',\n",
       " 'sop_05',\n",
       " 'EOO',\n",
       " 'EOL',\n",
       " 'sop_07',\n",
       " 'numerical',\n",
       " 'LinearSequenceMap',\n",
       " 'sop_06',\n",
       " 'sop_01',\n",
       " '3',\n",
       " '1',\n",
       " 'EOO',\n",
       " 'EOL',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero tokens: 26.6%\n"
     ]
    }
   ],
   "source": [
    "# number of non-padding tokens\n",
    "print(f\"Number of nonzero tokens: {(tokens > 0).sum() / tokens.size * 100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical sops: 44.7%\n",
      "Numerical sops: 55.3%\n",
      "Total sops: 405\n"
     ]
    }
   ],
   "source": [
    "# distribution of token types\n",
    "\n",
    "# encodings\n",
    "cat, num = (tokenizer.encode_token(t) for t in [\"categorical\", \"numerical\"])\n",
    "n_categorical = (tokens == cat).sum()\n",
    "n_numerical = (tokens == num).sum()\n",
    "total = n_categorical + n_numerical\n",
    "\n",
    "print(f\"Categorical sops: {100*n_categorical/total:0.1f}%\")\n",
    "print(f\"Numerical sops: {100*n_numerical/total:0.1f}%\")\n",
    "print(f\"Total sops: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS EOL\n",
      "sop_00 categorical Map lambda x: x - 1 sop_11_indices EOO sop_01 numerical Map lambda x: x != 3 sop_11_indices EOO EOL\n",
      "sop_02 categorical SelectAggregate sop_00 sop_00 EQ sop_10_tokens EOO EOL\n",
      "sop_03 categorical Map lambda x: not x sop_01 EOO sop_04 numerical Map lambda x: x sop_01 EOO EOL\n",
      "sop_05 numerical SelectAggregate sop_10_tokens sop_02 GT sop_01 EOO sop_06 numerical SelectAggregate sop_10_tokens sop_03 NEQ sop_04 EOO EOL\n",
      "sop_07 numerical LinearSequenceMap sop_06 sop_05 -3 3 EOO EOL\n",
      "EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(tokenizer.decode(data['tokens'][2])).replace(\"EOL \", \"EOL\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation counts:\n",
      "Map: 34.6%\n",
      "SequenceMap: 18.5%\n",
      "LinearSequenceMap: 16.0%\n",
      "SelectAggregate: 27.7%\n",
      "SelectorWidth: 3.2%\n",
      "Total SOps: 405\n"
     ]
    }
   ],
   "source": [
    "ops = tokenizer.encode(vocab.ops)\n",
    "op_counts = {vocab.vocab[op]: (tokens == op).sum() for op in ops}\n",
    "total = sum(op_counts.values())\n",
    "\n",
    "print(\"Operation counts:\")\n",
    "for op, count in op_counts.items():\n",
    "    print(f\"{op}: {100*count/total:.1f}%\")\n",
    "\n",
    "print(f\"Total SOps: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_inputs_and_outputs(\n",
    "    programs: list[rasp.SOp],\n",
    "    n_samples: int = 50,\n",
    "):\n",
    "    \"\"\"Generate test inputs and pass forward through programs to get outputs.\"\"\"\n",
    "    test_inputs = [rasp_utils.sample_test_input(rng, max_seq_len=5, \n",
    "                                    min_seq_len=5, vocab=set(range(10))) \n",
    "                for _ in range(n_samples)]\n",
    "    outputs = [[p(x) for x in test_inputs] for p in programs]\n",
    "    outputs = np.array(outputs, dtype=float)\n",
    "    outputs = np.nan_to_num(outputs, nan=0.0)\n",
    "    return test_inputs, outputs\n",
    "\n",
    "\n",
    "def test_low_var(outputs: list):\n",
    "    \"\"\"Test that sampled programs have a reasonable amount of variance wrt input\"\"\"\n",
    "    stds = np.std(outputs, axis=1).sum(axis=-1)  # std across test inputs; sum across output sequence\n",
    "    are_low_var = stds < 0.01\n",
    "    frac_low_var = sum(are_low_var) / len(stds)\n",
    "    print(f\"{frac_low_var*100}% of programs have low variance in output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.000000000000002% of programs have low variance in output.\n"
     ]
    }
   ],
   "source": [
    "programs = [tokenizer.detokenize(t) for t in tokens]\n",
    "inputs, outputs = get_test_inputs_and_outputs(programs)\n",
    "test_low_var(outputs)\n",
    "\n",
    "\n",
    "outputs_buffer = outputs.copy()\n",
    "\n",
    "program_data = []\n",
    "for i, p in enumerate(programs):\n",
    "    program_data.append({\n",
    "        \"program\": p,\n",
    "        \"outputs\": outputs[i],\n",
    "        \"std\": np.std(outputs[i], axis=0).sum(),\n",
    "    })\n",
    "\n",
    "\n",
    "# sort by std\n",
    "by_std = sorted(program_data, key=lambda x: np.std(x['outputs']))\n",
    "len(by_std)\n",
    "by_std = iter(by_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std: 0.0\n",
      "\n",
      "input:  [8, 6, 5, 2, 3]\n",
      "select_24 = Select(tokens, tokens, predicate=Comparison.TRUE)\n",
      "select_28 = Select(tokens, tokens, predicate=Comparison.EQ)\n",
      "sop_01_23 = rasp.numerical(Map(lambda x: x == 0, indices, simplify=False))    # output: [True, False, False, False, False]\n",
      "sop_00_27 = rasp.categorical(Aggregate(select_28, tokens))    # output: [8, 6, 5, 2, 3]\n",
      "sop_02_21 = rasp.numerical(Aggregate(select_24, sop_01_23))    # output: [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "sop_03_26 = rasp.categorical(SequenceMap(lambda x, y: x + y % 10, tokens, sop_00_27))    # output: [16, 12, 10, 4, 6]\n",
      "sop_04_25 = rasp.categorical(SequenceMap(lambda x, y: x - y, indices, sop_03_26))    # output: [-16, -11, -8, -1, -2]\n",
      "select_22 = Select(tokens, sop_04_25, predicate=Comparison.LEQ)\n",
      "sop_05_20 = rasp.numerical(Aggregate(select_22, sop_01_23))    # output: [0, 0, 0, 0, 0]\n",
      "sop_06_19 = rasp.numerical(LinearSequenceMap(sop_05_20, sop_02_21, -1, 2))    # output: [0.4, 0.4, 0.4, 0.4, 0.4]\n",
      "sop_07_18 = rasp.numerical(Map(lambda x: x < 3.5, sop_06_19, simplify=False))    # output: [True, True, True, True, True]\n",
      "\n",
      "sample outputs: [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "p = next(by_std)\n",
    "print(\"std:\", np.std(p['outputs']))\n",
    "print()\n",
    "print('input: ', inputs[0])\n",
    "rasp_utils.print_program(p['program'], test_input=inputs[0], full=True)\n",
    "print()\n",
    "print('sample outputs:', p['outputs'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50/50 unique model params (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates among tokens\n",
    "weights = data[\"weights\"]\n",
    "unique = defaultdict(list)\n",
    "duplicate_weights = []\n",
    "\n",
    "for i, w in enumerate(weights):\n",
    "    w = tuple(w.flatten().tolist())\n",
    "    if w in unique:\n",
    "        duplicate_weights.append(i)\n",
    "    \n",
    "    unique[w].append(i)\n",
    "\n",
    "print(f\"Found {len(unique)}/{len(weights)} unique model params \"\n",
    "      f\"({100 * len(unique) / len(weights):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent padding: 64.7%\n",
      "percent zero: 34.8%\n",
      "left over: 0.5%\n"
     ]
    }
   ],
   "source": [
    "print(f\"percent padding: {100 * (weights == 0.05).sum() / weights.size:0.1f}%\")\n",
    "print(f\"percent zero: {100 * (weights == 0).sum() / weights.size:0.1f}%\")\n",
    "print(f\"left over: {100 * np.logical_and(weights != 0, weights != 0.05).sum() / weights.size:0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent padding: 56.4%\n",
      "percent zero: 43.0%\n",
      "left over: 0.5%\n"
     ]
    }
   ],
   "source": [
    "def get_percentages(idx):\n",
    "    w = data[\"weights\"][idx]\n",
    "\n",
    "    print(f\"percent padding: {100 * (w == 0.05).sum() / w.size:0.1f}%\")\n",
    "    print(f\"percent zero: {100 * (w == 0).sum() / w.size:0.1f}%\")\n",
    "    print(f\"left over: {100 * np.logical_and(w != 0, w != 0.05).sum() / w.size:0.1f}%\")\n",
    "\n",
    "\n",
    "def plot_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    w = w.flatten()\n",
    "\n",
    "    plt.plot(w, \".\")\n",
    "    plt.yscale(\"symlog\", linthresh=0.1)\n",
    "\n",
    "    print(\" \".join(tokenizer.decode(t)))\n",
    "\n",
    "\n",
    "def imshow_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    _, d_model = w.shape\n",
    "    w = w.flatten()\n",
    "\n",
    "    is_padding = w == 0.05\n",
    "    first_padding_idx = is_padding.tolist().index(True)\n",
    "    idx = first_padding_idx + (d_model - first_padding_idx % d_model)\n",
    "    reshaped_w = w[:idx].reshape(-1, d_model)\n",
    "    reshaped_w[reshaped_w == 0] = np.nan\n",
    "    plt.imshow(reshaped_w, aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "get_percentages(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "#plot_datapoint(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = data[\"weights\"][idx]\n",
    "w = data_utils.symlog(w, linear_thresh = 2.0)\n",
    "w = w.flatten()\n",
    "#plt.plot(w, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dupe_idx \u001b[38;5;241m=\u001b[39m \u001b[43mduplicate_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m dupe_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(weights[dupe_idx]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      3\u001b[0m unique[dupe_w]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dupe_idx = duplicate_weights[0]\n",
    "dupe_w = tuple(weights[dupe_idx].flatten().tolist())\n",
    "unique[dupe_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dupe_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a, b \u001b[38;5;241m=\u001b[39m unique[\u001b[43mdupe_w\u001b[49m][\u001b[38;5;241m0\u001b[39m], unique[dupe_w][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokenizer\u001b[38;5;241m.\u001b[39mdecode(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m][a]))\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEOL \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEOL\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m PAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dupe_w' is not defined"
     ]
    }
   ],
   "source": [
    "a, b = unique[dupe_w][0], unique[dupe_w][1]\n",
    "print(\" \".join(tokenizer.decode(data['tokens'][a])).replace(\"EOL \", \"EOL\\n\").replace(\" PAD\", \"\"))\n",
    "print()\n",
    "print(\" \".join(tokenizer.decode(data['tokens'][b])).replace(\"EOL \", \"EOL\\n\").replace(\" PAD\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dupe_idx in duplicate_weights:\n",
    "    w = weights[dupe_idx]\n",
    "    t = tokens[dupe_idx]\n",
    "\n",
    "    duplicates = unique[tuple(w.flatten().tolist())]\n",
    "\n",
    "    if not all([np.all(tokens[i] == tokens[dupe_idx]) for i in duplicates]):\n",
    "        print(f\"dupe idx: {dupe_idx}\")\n",
    "        print(\"Found duplicates with different tokens:\")\n",
    "        for i in duplicates:\n",
    "            print(\" \".join(tokenizer.decode(tokens[i])))\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for close duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# \n",
    "# for w in tqdm(weights):\n",
    "#     close = [np.allclose(w, u) for u in unique.values()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
