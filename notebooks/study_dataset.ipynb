{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import sys\n",
    "from collections import defaultdict \n",
    "import jax\n",
    "import flax\n",
    "import chex\n",
    "from jaxtyping import ArrayLike\n",
    "from typing import Union, TypeVar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from tracr.compiler.validating import validate\n",
    "from tracr.rasp.rasp import Map, SequenceMap, LinearSequenceMap, Select, Aggregate, Comparison, SelectorWidth, indices, tokens \n",
    "from tracr.rasp import rasp\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler.assemble import AssembledTransformerModel\n",
    "from tracr.compiler.craft_model_to_transformer import NoTokensError\n",
    "from tracr.compiler.basis_inference import InvalidValueSetError\n",
    "from tracr.compiler import rasp_to_graph\n",
    "\n",
    "\n",
    "from decompile_tracr.dataset import lib\n",
    "from decompile_tracr.dataset import data_utils\n",
    "from decompile_tracr.dataset import config\n",
    "from decompile_tracr.tokenizing import tokenizer\n",
    "from decompile_tracr.tokenizing import vocab\n",
    "from decompile_tracr.sampling import sampling\n",
    "from decompile_tracr.sampling import rasp_utils\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_DATA_RATIO = 0.1\n",
    "MAX_RASP_LENGTH = config.MAX_RASP_LENGTH\n",
    "MAX_WEIGHTS_LENGTH = config.MAX_WEIGHTS_LENGTH\n",
    "FULL_DATA_DIR = config.full_dataset_dir\n",
    "ALL_LAYERS_MULTIPLIER = 15\n",
    "split_layers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_utils.load_and_process_data(\n",
    "    rng=rng,\n",
    "    loaddir=FULL_DATA_DIR,\n",
    "    max_data=5000,\n",
    "    shuffle=True,\n",
    "    name=\"train\",\n",
    "    d_model=128,\n",
    "    max_rasp_len=MAX_RASP_LENGTH if split_layers else MAX_RASP_LENGTH * ALL_LAYERS_MULTIPLIER,\n",
    "    max_weights_len=MAX_WEIGHTS_LENGTH if split_layers else MAX_WEIGHTS_LENGTH * ALL_LAYERS_MULTIPLIER,\n",
    "    split_layers=split_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"keys:\", data.keys())\n",
    "print(\"data shapes:\", {k: v.shape for k, v in data.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates among tokens\n",
    "tokens = data[\"tokens\"]\n",
    "unique_tokens = defaultdict(list)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    t = tuple(token.tolist())\n",
    "    unique_tokens[t].append(i)\n",
    "\n",
    "print(f\"Found {len(unique_tokens)}/{len(tokens)} unique tokens \"\n",
    "      f\"({100 * len(unique_tokens) / len(tokens):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of non-padding tokens\n",
    "print(f\"Number of nonzero tokens: {(tokens > 0).sum() / tokens.size * 100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of token types\n",
    "\n",
    "# encodings\n",
    "cat, num = tokenizer.encode([\"categorical\", \"numerical\"])\n",
    "n_categorical = (tokens == cat).sum()\n",
    "n_numerical = (tokens == num).sum()\n",
    "total = n_categorical + n_numerical\n",
    "\n",
    "print(f\"Categorical sops: {100*n_categorical/total:0.1f}%\")\n",
    "print(f\"Numerical sops: {100*n_numerical/total:0.1f}%\")\n",
    "print(f\"Total sops: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = tokenizer.encode(vocab.ops)\n",
    "op_counts = {vocab.vocab[op]: (tokens == op).sum() for op in ops}\n",
    "total = sum(op_counts.values())\n",
    "\n",
    "print(\"Operation counts:\")\n",
    "for op, count in op_counts.items():\n",
    "    print(f\"{op}: {100*count/total:.1f}%\")\n",
    "\n",
    "print(f\"Total SOps: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates among tokens\n",
    "weights = data[\"weights\"]\n",
    "unique = defaultdict(list)\n",
    "duplicate_weights = []\n",
    "\n",
    "for i, w in enumerate(weights):\n",
    "    w = tuple(w.flatten().tolist())\n",
    "    if w in unique:\n",
    "        duplicate_weights.append(i)\n",
    "    \n",
    "    unique[w].append(i)\n",
    "\n",
    "print(f\"Found {len(unique)}/{len(weights)} unique model params \"\n",
    "      f\"({100 * len(unique) / len(weights):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"percent padding: {100 * (weights == 0.05).sum() / weights.size:0.1f}%\")\n",
    "print(f\"percent zero: {100 * (weights == 0).sum() / weights.size:0.1f}%\")\n",
    "print(f\"left over: {100 * np.logical_and(weights != 0, weights != 0.05).sum() / weights.size:0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentages(idx):\n",
    "    w = data[\"weights\"][idx]\n",
    "\n",
    "    print(f\"percent padding: {100 * (w == 0.05).sum() / w.size:0.1f}%\")\n",
    "    print(f\"percent zero: {100 * (w == 0).sum() / w.size:0.1f}%\")\n",
    "    print(f\"left over: {100 * np.logical_and(w != 0, w != 0.05).sum() / w.size:0.1f}%\")\n",
    "\n",
    "\n",
    "def plot_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    w = w.flatten()\n",
    "\n",
    "    plt.plot(w, \".\")\n",
    "    plt.yscale(\"symlog\", linthresh=0.1)\n",
    "\n",
    "    print(\" \".join(tokenizer.decode(t)))\n",
    "\n",
    "\n",
    "def imshow_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    _, d_model = w.shape\n",
    "    w = w.flatten()\n",
    "\n",
    "    is_padding = w == 0.05\n",
    "    first_padding_idx = is_padding.tolist().index(True)\n",
    "    idx = first_padding_idx + (d_model - first_padding_idx % d_model)\n",
    "    reshaped_w = w[:idx].reshape(-1, d_model)\n",
    "    reshaped_w[reshaped_w == 0] = np.nan\n",
    "    plt.imshow(reshaped_w, aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "get_percentages(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "#plot_datapoint(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dupe_idx in duplicate_weights:\n",
    "    w = weights[dupe_idx]\n",
    "    t = tokens[dupe_idx]\n",
    "\n",
    "    duplicates = unique[tuple(w.flatten().tolist())]\n",
    "\n",
    "    if not all([np.all(tokens[i] == tokens[dupe_idx]) for i in duplicates]):\n",
    "        print(f\"dupe idx: {dupe_idx}\")\n",
    "        print(\"Found duplicates with different tokens:\")\n",
    "        for i in duplicates:\n",
    "            print(\" \".join(tokenizer.decode(tokens[i])))\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for close duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for w in tqdm(weights):\n",
    "    close = [np.allclose(w, u) for u in unique.values()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
