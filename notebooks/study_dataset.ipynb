{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import sys\n",
    "from collections import defaultdict \n",
    "import jax\n",
    "import flax\n",
    "import chex\n",
    "from jaxtyping import ArrayLike\n",
    "from typing import Union, TypeVar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from tracr.compiler.validating import validate\n",
    "from tracr.rasp.rasp import Map, SequenceMap, LinearSequenceMap, Select, Aggregate, Comparison, SelectorWidth, indices, tokens \n",
    "from tracr.rasp import rasp\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler.assemble import AssembledTransformerModel\n",
    "from tracr.compiler.craft_model_to_transformer import NoTokensError\n",
    "from tracr.compiler.basis_inference import InvalidValueSetError\n",
    "from tracr.compiler import rasp_to_graph\n",
    "\n",
    "\n",
    "from decompile_tracr.dataset import lib\n",
    "from decompile_tracr.dataset import data_utils\n",
    "from decompile_tracr.dataset import config\n",
    "from decompile_tracr.tokenizing import tokenizer\n",
    "from decompile_tracr.tokenizing import vocab\n",
    "from decompile_tracr.sampling import sampling\n",
    "from decompile_tracr.sampling import rasp_utils\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_DATA_RATIO = 0.1\n",
    "MAX_RASP_LENGTH = config.MAX_RASP_LENGTH\n",
    "MAX_WEIGHTS_LENGTH = config.MAX_WEIGHTS_LENGTH\n",
    "FULL_DATA_DIR = config.full_dataset_dir\n",
    "ALL_LAYERS_MULTIPLIER = 15\n",
    "split_layers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-25 15:06:12 - [INFO]: Loading data from /home/lauro/projects/meta-models/decompile-tracr/data/full.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-25 15:06:15 - [INFO]: load_batches: Loaded 5037 >= 5000 datapoints. Stopping and truncating to 5000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 15:06:17.132942: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "data = data_utils.load_and_process_data(\n",
    "    rng=rng,\n",
    "    loaddir=FULL_DATA_DIR,\n",
    "    max_data=5000,\n",
    "    shuffle=True,\n",
    "    name=\"train\",\n",
    "    d_model=128,\n",
    "    max_rasp_len=MAX_RASP_LENGTH if split_layers else MAX_RASP_LENGTH * ALL_LAYERS_MULTIPLIER,\n",
    "    max_weights_len=MAX_WEIGHTS_LENGTH if split_layers else MAX_WEIGHTS_LENGTH * ALL_LAYERS_MULTIPLIER,\n",
    "    split_layers=split_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['tokens', 'weights', 'program_id', 'n_sops', 'n_layers'])\n",
      "data shapes: {'tokens': (96, 480), 'weights': (96, 960, 128), 'program_id': (96,), 'n_sops': (96,), 'n_layers': (96,)}\n"
     ]
    }
   ],
   "source": [
    "print(\"keys:\", data.keys())\n",
    "print(\"data shapes:\", {k: v.shape for k, v in data.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96/96 unique tokens (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates among tokens\n",
    "tokens = data[\"tokens\"]\n",
    "unique_tokens = defaultdict(list)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    t = tuple(token.tolist())\n",
    "    unique_tokens[t].append(i)\n",
    "\n",
    "print(f\"Found {len(unique_tokens)}/{len(tokens)} unique tokens \"\n",
    "      f\"({100 * len(unique_tokens) / len(tokens):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero tokens: 6.3%\n"
     ]
    }
   ],
   "source": [
    "# number of non-padding tokens\n",
    "print(f\"Number of nonzero tokens: {(tokens > 0).sum() / tokens.size * 100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical sops: 84.3%\n",
      "Numerical sops: 15.7%\n",
      "Total sops: 306\n"
     ]
    }
   ],
   "source": [
    "# distribution of token types\n",
    "\n",
    "# encodings\n",
    "cat, num = tokenizer.encode([\"categorical\", \"numerical\"])\n",
    "n_categorical = (tokens == cat).sum()\n",
    "n_numerical = (tokens == num).sum()\n",
    "total = n_categorical + n_numerical\n",
    "\n",
    "print(f\"Categorical sops: {100*n_categorical/total:0.1f}%\")\n",
    "print(f\"Numerical sops: {100*n_numerical/total:0.1f}%\")\n",
    "print(f\"Total sops: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation counts:\n",
      "Map: 14.7%\n",
      "SequenceMap: 30.4%\n",
      "LinearSequenceMap: 1.0%\n",
      "SelectAggregate: 18.3%\n",
      "SelectorWidth: 35.6%\n",
      "Total SOps: 306\n"
     ]
    }
   ],
   "source": [
    "ops = tokenizer.encode(vocab.ops)\n",
    "op_counts = {vocab.vocab[op]: (tokens == op).sum() for op in ops}\n",
    "total = sum(op_counts.values())\n",
    "\n",
    "print(\"Operation counts:\")\n",
    "for op, count in op_counts.items():\n",
    "    print(f\"{op}: {100*count/total:.1f}%\")\n",
    "\n",
    "print(f\"Total SOps: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96/96 unique model params (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates among tokens\n",
    "weights = data[\"weights\"]\n",
    "unique = defaultdict(list)\n",
    "duplicate_weights = []\n",
    "\n",
    "for i, w in enumerate(weights):\n",
    "    w = tuple(w.flatten().tolist())\n",
    "    if w in unique:\n",
    "        duplicate_weights.append(i)\n",
    "    \n",
    "    unique[w].append(i)\n",
    "\n",
    "print(f\"Found {len(unique)}/{len(weights)} unique model params \"\n",
    "      f\"({100 * len(unique) / len(weights):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent padding: 93.7%\n",
      "percent zero: 6.1%\n",
      "left over: 0.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"percent padding: {100 * (weights == 0.05).sum() / weights.size:0.1f}%\")\n",
    "print(f\"percent zero: {100 * (weights == 0).sum() / weights.size:0.1f}%\")\n",
    "print(f\"left over: {100 * np.logical_and(weights != 0, weights != 0.05).sum() / weights.size:0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent padding: 93.1%\n",
      "percent zero: 6.7%\n",
      "left over: 0.2%\n"
     ]
    }
   ],
   "source": [
    "def get_percentages(idx):\n",
    "    w = data[\"weights\"][idx]\n",
    "\n",
    "    print(f\"percent padding: {100 * (w == 0.05).sum() / w.size:0.1f}%\")\n",
    "    print(f\"percent zero: {100 * (w == 0).sum() / w.size:0.1f}%\")\n",
    "    print(f\"left over: {100 * np.logical_and(w != 0, w != 0.05).sum() / w.size:0.1f}%\")\n",
    "\n",
    "\n",
    "def plot_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    w = w.flatten()\n",
    "\n",
    "    plt.plot(w, \".\")\n",
    "    plt.yscale(\"symlog\", linthresh=0.1)\n",
    "\n",
    "    print(\" \".join(tokenizer.decode(t)))\n",
    "\n",
    "\n",
    "def imshow_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    _, d_model = w.shape\n",
    "    w = w.flatten()\n",
    "\n",
    "    is_padding = w == 0.05\n",
    "    first_padding_idx = is_padding.tolist().index(True)\n",
    "    idx = first_padding_idx + (d_model - first_padding_idx % d_model)\n",
    "    reshaped_w = w[:idx].reshape(-1, d_model)\n",
    "    reshaped_w[reshaped_w == 0] = np.nan\n",
    "    plt.imshow(reshaped_w, aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "get_percentages(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "#plot_datapoint(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dupe_idx in duplicate_weights:\n",
    "    w = weights[dupe_idx]\n",
    "    t = tokens[dupe_idx]\n",
    "\n",
    "    duplicates = unique[tuple(w.flatten().tolist())]\n",
    "\n",
    "    if not all([np.all(tokens[i] == tokens[dupe_idx]) for i in duplicates]):\n",
    "        print(f\"dupe idx: {dupe_idx}\")\n",
    "        print(\"Found duplicates with different tokens:\")\n",
    "        for i in duplicates:\n",
    "            print(\" \".join(tokenizer.decode(tokens[i])))\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for close duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:04<00:00, 20.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for w in tqdm(weights):\n",
    "    close = [np.allclose(w, u) for u in unique.values()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
