{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import sys\n",
    "from collections import defaultdict \n",
    "import jax\n",
    "import flax\n",
    "import chex\n",
    "from jaxtyping import ArrayLike\n",
    "from typing import Union, TypeVar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from tracr.compiler.validating import validate\n",
    "from tracr.rasp.rasp import Map, SequenceMap, LinearSequenceMap, Select, Aggregate, Comparison, SelectorWidth, indices, tokens \n",
    "from tracr.rasp import rasp\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler.assemble import AssembledTransformerModel\n",
    "from tracr.compiler.craft_model_to_transformer import NoTokensError\n",
    "from tracr.compiler.basis_inference import InvalidValueSetError\n",
    "from tracr.compiler import rasp_to_graph\n",
    "\n",
    "\n",
    "from decompile_tracr.dataset import lib\n",
    "from decompile_tracr.dataset import data_utils\n",
    "from decompile_tracr.dataset import config\n",
    "from decompile_tracr.tokenizing import tokenizer\n",
    "from decompile_tracr.tokenizing import vocab\n",
    "from decompile_tracr.sampling import sampling\n",
    "from decompile_tracr.sampling import rasp_utils\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_DATA_RATIO = 0.1\n",
    "MAX_RASP_LENGTH = config.MAX_RASP_LENGTH\n",
    "MAX_WEIGHTS_LENGTH = config.MAX_WEIGHTS_LENGTH\n",
    "FULL_DATA_DIR = config.full_dataset_dir\n",
    "ALL_LAYERS_MULTIPLIER = 15\n",
    "split_layers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-12 12:11:19 - [INFO]: Loading data from /home/lauro/projects/meta-models/decompile-tracr/data/full.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-12 12:11:19 - [INFO]: load_batches: Loaded 1079 >= 1000 datapoints. Stopping and truncating to 1000 datapoints.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 12:11:19.821075: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2024-04-12 12:11:19.821160: E external/xla/xla/stream_executor/cuda/cuda_diagnostics.cc:256] kernel version 535.161.7 does not match DSO version 535.171.4 -- cannot find working devices in this configuration\n",
      "CUDA backend failed to initialize: FAILED_PRECONDITION: No visible GPU devices. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "data = data_utils.load_dataset_for_model_input(\n",
    "    rng=rng,\n",
    "    loaddir=FULL_DATA_DIR,\n",
    "    max_data=1000,\n",
    "    shuffle=True,\n",
    "    d_model=128,\n",
    "    max_rasp_len=MAX_RASP_LENGTH if split_layers else MAX_RASP_LENGTH * ALL_LAYERS_MULTIPLIER,\n",
    "    max_weights_len=MAX_WEIGHTS_LENGTH if split_layers else MAX_WEIGHTS_LENGTH * ALL_LAYERS_MULTIPLIER,\n",
    "    split_layers=split_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: ['tokens', 'weights', 'n_sops', 'program_id', 'n_layers']\n",
      "data shapes: {'tokens': (1000, 1920), 'weights': (1000, 1920, 128), 'n_sops': (1000,), 'program_id': (1000,), 'n_layers': (1000,)}\n"
     ]
    }
   ],
   "source": [
    "print(\"keys:\", list(data.keys()))\n",
    "print(\"data shapes:\", {k: v.shape for k, v in data.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000/1000 unique tokens (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates among tokens\n",
    "tokens = data[\"tokens\"]\n",
    "unique_tokens = defaultdict(list)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    t = tuple(token.tolist())\n",
    "    unique_tokens[t].append(i)\n",
    "\n",
    "print(f\"Found {len(unique_tokens)}/{len(tokens)} unique tokens \"\n",
    "      f\"({100 * len(unique_tokens) / len(tokens):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero tokens: 1.5%\n"
     ]
    }
   ],
   "source": [
    "# number of non-padding tokens\n",
    "print(f\"Number of nonzero tokens: {(tokens > 0).sum() / tokens.size * 100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical sops: 64.7%\n",
      "Numerical sops: 35.3%\n",
      "Total sops: 3,223\n"
     ]
    }
   ],
   "source": [
    "# distribution of token types\n",
    "\n",
    "# encodings\n",
    "cat, num = (tokenizer.encode(t) for t in [\"categorical\", \"numerical\"])\n",
    "n_categorical = (tokens == cat).sum()\n",
    "n_numerical = (tokens == num).sum()\n",
    "total = n_categorical + n_numerical\n",
    "\n",
    "print(f\"Categorical sops: {100*n_categorical/total:0.1f}%\")\n",
    "print(f\"Numerical sops: {100*n_numerical/total:0.1f}%\")\n",
    "print(f\"Total sops: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation counts:\n",
      "Map: 33.3%\n",
      "SequenceMap: 31.1%\n",
      "LinearSequenceMap: 3.6%\n",
      "SelectAggregate: 29.8%\n",
      "SelectorWidth: 2.2%\n",
      "Total SOps: 3,223\n"
     ]
    }
   ],
   "source": [
    "ops = (tokenizer.encode(t) for t in vocab.ops)\n",
    "op_counts = {vocab.vocab[op]: (tokens == op).sum() for op in ops}\n",
    "total = sum(op_counts.values())\n",
    "\n",
    "print(\"Operation counts:\")\n",
    "for op, count in op_counts.items():\n",
    "    print(f\"{op}: {100*count/total:.1f}%\")\n",
    "\n",
    "print(f\"Total SOps: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_inputs_and_outputs(\n",
    "    programs: list[rasp.SOp],\n",
    "    n_samples: int = 50,\n",
    "):\n",
    "    \"\"\"Generate test inputs and pass forward through programs to get outputs.\"\"\"\n",
    "    test_inputs = [rasp_utils.sample_test_input(rng, max_seq_len=5, \n",
    "                                    min_seq_len=5, vocab=set(range(10))) \n",
    "                for _ in range(n_samples)]\n",
    "    outputs = [[p(x) for x in test_inputs] for p in programs]\n",
    "    outputs = np.array(outputs, dtype=float)\n",
    "    outputs = np.nan_to_num(outputs, nan=0.0)\n",
    "    return test_inputs, outputs\n",
    "\n",
    "\n",
    "def test_low_var(outputs: list):\n",
    "    \"\"\"Test that sampled programs have a reasonable amount of variance wrt input\"\"\"\n",
    "    stds = np.std(outputs, axis=1).sum(axis=-1)  # std across test inputs; sum across output sequence\n",
    "    are_low_var = stds < 0.01\n",
    "    frac_low_var = sum(are_low_var) / len(stds)\n",
    "    print(f\"{frac_low_var*100}% of programs have low variance in output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4% of programs have low variance in output.\n"
     ]
    }
   ],
   "source": [
    "programs = [tokenizer.detokenize(t) for t in tokens]\n",
    "inputs, outputs = get_test_inputs_and_outputs(programs)\n",
    "test_low_var(outputs)\n",
    "\n",
    "\n",
    "outputs_buffer = outputs.copy()\n",
    "\n",
    "program_data = []\n",
    "for i, p in enumerate(programs):\n",
    "    program_data.append({\n",
    "        \"program\": p,\n",
    "        \"outputs\": outputs[i],\n",
    "        \"std\": np.std(outputs[i], axis=0).sum(),\n",
    "    })\n",
    "\n",
    "\n",
    "# sort by std\n",
    "by_std = sorted(program_data, key=lambda x: np.std(x['outputs']))\n",
    "len(by_std)\n",
    "by_std = iter(by_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std: 0.0\n",
      "\n",
      "input:  [9, 3, 2, 4, 1]\n",
      "sop_0_20 = rasp.numerical(Map(lambda x: x == 1, indices))    # output: [False, True, False, False, False]\n",
      "sop_1_21 = rasp.categorical(SequenceMap(lambda x, y: x - y, tokens, indices))    # output: [9, 2, 0, 1, -3]\n",
      "select_19 = Select(sop_1_21, tokens, predicate=Comparison.TRUE)\n",
      "sop_2_18 = rasp.numerical(Aggregate(select_19, sop_0_20))    # output: [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "\n",
      "sample outputs: [[0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]]\n"
     ]
    }
   ],
   "source": [
    "p = next(by_std)\n",
    "print(\"std:\", np.std(p['outputs']))\n",
    "print()\n",
    "print('input: ', inputs[0])\n",
    "rasp_utils.print_program(p['program'], test_input=inputs[0], full=True)\n",
    "print()\n",
    "print('sample outputs:', p['outputs'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300/1000 unique model params (30.00%)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates among tokens\n",
    "weights = data[\"weights\"]\n",
    "unique = defaultdict(list)\n",
    "duplicate_weights = []\n",
    "\n",
    "for i, w in enumerate(weights[:300]):\n",
    "    w = tuple(w.flatten().tolist())\n",
    "    if w in unique:\n",
    "        duplicate_weights.append(i)\n",
    "    \n",
    "    unique[w].append(i)\n",
    "\n",
    "print(f\"Found {len(unique)}/{len(weights)} unique model params \"\n",
    "      f\"({100 * len(unique) / len(weights):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent padding: 97.6%\n",
      "percent zero: 2.3%\n",
      "left over: 0.1%\n"
     ]
    }
   ],
   "source": [
    "print(f\"percent padding: {100 * (weights == 0.05).sum() / weights.size:0.1f}%\")\n",
    "print(f\"percent zero: {100 * (weights == 0).sum() / weights.size:0.1f}%\")\n",
    "print(f\"left over: {100 * np.logical_and(weights != 0, weights != 0.05).sum() / weights.size:0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent padding: 98.0%\n",
      "percent zero: 1.9%\n",
      "left over: 0.1%\n"
     ]
    }
   ],
   "source": [
    "def get_percentages(idx):\n",
    "    w = data[\"weights\"][idx]\n",
    "\n",
    "    print(f\"percent padding: {100 * (w == 0.05).sum() / w.size:0.1f}%\")\n",
    "    print(f\"percent zero: {100 * (w == 0).sum() / w.size:0.1f}%\")\n",
    "    print(f\"left over: {100 * np.logical_and(w != 0, w != 0.05).sum() / w.size:0.1f}%\")\n",
    "\n",
    "\n",
    "def plot_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    w = w.flatten()\n",
    "\n",
    "    plt.plot(w, \".\")\n",
    "    plt.yscale(\"symlog\", linthresh=0.1)\n",
    "\n",
    "    print(\" \".join(tokenizer.decode(t)))\n",
    "\n",
    "\n",
    "def imshow_datapoint(idx):\n",
    "    t = tokens[idx]\n",
    "    w = data[\"weights\"][idx]\n",
    "    _, d_model = w.shape\n",
    "    w = w.flatten()\n",
    "\n",
    "    is_padding = w == 0.05\n",
    "    first_padding_idx = is_padding.tolist().index(True)\n",
    "    idx = first_padding_idx + (d_model - first_padding_idx % d_model)\n",
    "    reshaped_w = w[:idx].reshape(-1, d_model)\n",
    "    reshaped_w[reshaped_w == 0] = np.nan\n",
    "    plt.imshow(reshaped_w, aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "get_percentages(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "#plot_datapoint(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dupe_idx in duplicate_weights:\n",
    "    w = weights[dupe_idx]\n",
    "    t = tokens[dupe_idx]\n",
    "\n",
    "    duplicates = unique[tuple(w.flatten().tolist())]\n",
    "\n",
    "    if not all([np.all(tokens[i] == tokens[dupe_idx]) for i in duplicates]):\n",
    "        print(f\"dupe idx: {dupe_idx}\")\n",
    "        print(\"Found duplicates with different tokens:\")\n",
    "        for i in duplicates:\n",
    "            print(\" \".join((tokenizer.decode(t) for t in tokens[i])))\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for close duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# \n",
    "# for w in tqdm(weights):\n",
    "#     close = [np.allclose(w, u) for u in unique.values()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
