{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "from collections import defaultdict \n",
    "from typing import Union, TypeVar\n",
    "import h5py\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import flax\n",
    "import chex\n",
    "from jaxtyping import ArrayLike\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "from tracr.compiler.validating import validate\n",
    "from tracr.rasp.rasp import Map, SequenceMap, LinearSequenceMap, Select, Aggregate, Comparison, SelectorWidth, indices, tokens \n",
    "from tracr.rasp import rasp\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler.assemble import AssembledTransformerModel\n",
    "from tracr.compiler.craft_model_to_transformer import NoTokensError\n",
    "from tracr.compiler.basis_inference import InvalidValueSetError\n",
    "from tracr.compiler import craft_graph_to_model\n",
    "from tracr.compiler import rasp_to_graph\n",
    "from tracr.compiler import lib as tracr_lib\n",
    "from tracr.compiler import assemble\n",
    "from tracr.transformer import model\n",
    "\n",
    "from decompile_tracr.dataset import lib\n",
    "from decompile_tracr.dataset import data_utils\n",
    "from decompile_tracr.dataset import dataloading\n",
    "from decompile_tracr.dataset import config\n",
    "from decompile_tracr.dataset import compile as comp\n",
    "from decompile_tracr.tokenizing import tokenizer\n",
    "from decompile_tracr.tokenizing import vocab\n",
    "from decompile_tracr.sampling import sampling\n",
    "from decompile_tracr.sampling import rasp_utils\n",
    "from decompile_tracr.sampling.map_primitives import FunctionWithRepr\n",
    "from decompile_tracr.tokenizing.str_to_rasp import split_list\n",
    "from decompile_tracr.dataset.compile import get_weights\n",
    "from decompile_tracr.training.autoencoder import Autoencoder, get_loss_fn\n",
    "\n",
    "from metamodels_for_rasp.model import TransformerConfig, AddPositionEmbs, Encoder1DBlock, MLPBlock\n",
    "from metamodels_for_rasp.train import Updater\n",
    "\n",
    "\n",
    "def _compile(program):\n",
    "    return compiling.compile_rasp_to_model(\n",
    "        program,\n",
    "        vocab=set(range(5)),\n",
    "        max_seq_len=5,\n",
    "    )\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume m is shorthand for an AssembledModel, eg\n",
    "```\n",
    "m = _compile(program)\n",
    "```\n",
    "Then the following all commute:\n",
    "* m.apply: inputs --> out\n",
    "* m.forward: emb --> out\n",
    "* m.input_encoder.encode: inputs --> tokens\n",
    "* compiled_model.embed: tokens --> emb\n",
    "* transformer: emb --> out\n",
    "\n",
    "So overall we have\n",
    "* inputs --> tokens --> emb --> out\n",
    "* inputs --> out (via m.apply)\n",
    "* tokens --> out (via m.foward)\n",
    "* emb --> out (via transformer)\n",
    "\n",
    "ETA: that's only approximately true. The methods apply, forward, and transformer have different output types:\n",
    "* m.apply returns AssembledTransformerOutput\n",
    "* m.forward returns CompiledTransformerOutput\n",
    "* transformer returns TransformerOutput (included as attribute in the other two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tokenizer.detokenize(tokenizer.tokenize(sampling.sample(rng, 5)))\n",
    "m = _compile(p)\n",
    "d_model = m.params['token_embed']['embeddings'].shape[-1]\n",
    "print(\"d_model:\", d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.input_encoder.encode(['compiler_bos', 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def embed(tokens):\n",
    "    compiled_model = m.get_compiled_model()\n",
    "    return compiled_model.embed(tokens)\n",
    "\n",
    "\n",
    "e = embed.apply(m.params, np.array([1, 2, 3, 0]))\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def forward(tokens: ArrayLike):\n",
    "    \"\"\"tokens must be integer arrays\"\"\"\n",
    "    compiled_model = m.get_compiled_model()\n",
    "    return compiled_model(tokens, use_dropout=False)\n",
    "\n",
    "\n",
    "out = forward.apply(m.params, np.ones((5, 5), dtype=int))\n",
    "print(out.keys())\n",
    "print(out.transformer_output.keys())\n",
    "print()\n",
    "print(type(out))\n",
    "print([type(o) for o in out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def transformer(embeddings: ArrayLike):\n",
    "    \"\"\"embeddings must be float arrays of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    compiled_model = m.get_compiled_model()\n",
    "    return compiled_model.transformer(\n",
    "        embeddings, jnp.ones(embeddings.shape[:-1]), use_dropout=False)\n",
    "\n",
    "\n",
    "seq = 4\n",
    "out = transformer.apply(m.params, np.ones((1, seq, d_model), dtype=float))\n",
    "out.output.shape\n",
    "type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm that everything commutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def _unembed(x):\n",
    "    cm = m.get_compiled_model()\n",
    "    return cm.unembed(x, use_unembed_argmax=cm.use_unembed_argmax)\n",
    "\n",
    "def unembed(x):\n",
    "    return _unembed.apply(m.params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"compiler_bos\"] + [1, 4, 3, 2, 4]\n",
    "tokens = np.array(m.input_encoder.encode(inputs))[None, :]\n",
    "embeddings = embed.apply(m.params, tokens)\n",
    "\n",
    "apply_out = m.apply(inputs)\n",
    "forward_out = m.forward(m.params, tokens)\n",
    "transformer_out = transformer.apply(m.params, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = apply_out.transformer_output\n",
    "assert (np.all(x == forward_out.transformer_output.output) &\n",
    "        np.all(x == transformer_out.output))\n",
    "print(\"Embedded output shape:\", x.shape)  # last layer of residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after unembed:\n",
    "y = apply_out.unembedded\n",
    "assert (np.all(y == forward_out.unembedded_output) &\n",
    "        np.all(y == unembed(transformer_out.output)))\n",
    "print(\"Unembedded output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(transformer_out.residuals[0] == transformer_out.input_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
